{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of varying trimming thresholds of microbial communities of known composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script for the analysis and figure generation for 515F/926R 16S and 18S mock communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import libraries in python3 kernel\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "!conda install --yes --prefix {sys.prefix} boto\n",
    "import boto\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import skbio\n",
    "#import fastcluster #this package makes skbio run faster clustermaps but can be tricky with missing values from pairwise comparisons\n",
    "from functools import reduce\n",
    "!conda install --yes --prefix {sys.prefix} biopython\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "from Bio.SeqUtils import GC\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import statistics\n",
    "import itertools as it\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from qiime2 import Artifact\n",
    "import tempfile\n",
    "import zipfile\n",
    "import yaml\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__consolidate_tables__ creates a dataframe of all the merged feature tables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special thanks to Alex Manuele https://github.com/alexmanuele\n",
    "def consolidate_tables(community):\n",
    "    if community == \"16S\":\n",
    "        comm_id, comm = '16S', '02-PROKs'\n",
    "    if community == \"18S\":\n",
    "        comm_id, comm = '18S','02-EUKs'\n",
    "\n",
    "    table_list = glob.glob('{0}/*/03-DADA2d/table.qza'.format(comm+'/all_trims/'))\n",
    "    print(\"Found all tables\")\n",
    "\n",
    "    dataframes = []  \n",
    "    for table_path in table_list:\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            #load table, dump contents to tempdir\n",
    "            table = Artifact.load(table_path)\n",
    "            #Make sure the tables are all FeatureFrequency type\n",
    "            assert str(table.type) == 'FeatureTable[Frequency]', \"{0}: Expected FeatureTable[Frequency], got {1}\".format(table_path, table.type)\n",
    "            Artifact.extract(table_path, tempdir)\n",
    "            #get the provenance form the tempdir and format it for DF\n",
    "            prov = '{0}/{1}/provenance/'.format(tempdir, table.uuid)\n",
    "            action = yaml.load(open(\"{0}action/action.yaml\".format(prov), 'r'), Loader=yaml.BaseLoader)\n",
    "            paramlist = action['action']['parameters']\n",
    "            paramlist.append({'table_uuid': \"{}\".format(table.uuid)})\n",
    "            paramdict = {}\n",
    "            for record in paramlist:\n",
    "                paramdict.update(record)\n",
    "\n",
    "            # Get the data into a dataframe\n",
    "              #Biom data\n",
    "            df = table.view(pd.DataFrame).unstack().reset_index()\n",
    "            df.columns = ['feature_id', 'sample_name', 'feature_frequency']\n",
    "            df['table_uuid'] = [\"{}\".format(table.uuid)] * df.shape[0]\n",
    "              #param data\n",
    "            pdf = pd.DataFrame.from_records([paramdict])\n",
    "              #merge params into main df\n",
    "            df = df.merge(pdf, on='table_uuid')\n",
    "\n",
    "\n",
    "            #I like having these columns as the last three. Makes it more readable\n",
    "            cols = df.columns.tolist()\n",
    "            reorder = ['sample_name', 'feature_id', 'feature_frequency']\n",
    "            for val in reorder:\n",
    "                cols.append(cols.pop(cols.index(val)))\n",
    "            df = df[cols]\n",
    "            df['table_path'] = [table_path] * df.shape[0]\n",
    "            dataframes.append(df)\n",
    "\n",
    "    #Stick all the dataframes together\n",
    "    #outputfile=\"merged_all_tables.tsv\"\n",
    "    df = pd.concat(dataframes)\n",
    "    df.to_csv(comm+'/merged_all_tables.tsv', sep='\\t', index=False)\n",
    "    print(\"Success.\")\n",
    "    return df, comm, comm_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found all tables\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "df, comm, comm_id = consolidate_tables('16S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__merge_metadata__ adds the metadata to the merged feature tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_metadata():\n",
    "    #df = pd.read_csv('02-PROKs/'+'/merged_all_tables.tsv', sep='\\t')\n",
    "\n",
    "    tables = df[['sample_name', 'feature_id', 'feature_frequency']].copy()\n",
    "    tables.rename(columns={'sample_name':'file'}, inplace=True)\n",
    "    manifest = pd.read_csv('MANIFEST.tsv', sep='\\t')\n",
    "    manifest['file'] = [s.split('SPOT_USC_2/')[1] for s in manifest['absolute-filepath']]\n",
    "    manifest['file'] = [s.split('.R')[0] for s in manifest['file']]\n",
    "    manifest = manifest.drop(columns = ['absolute-filepath', 'direction'])\n",
    "    manifest.drop_duplicates()\n",
    "    merged = pd.merge(tables,manifest, on='file')\n",
    "    merged = merged.drop(columns = ['file'])\n",
    "    merged = merged.drop_duplicates() \n",
    "    print('Set up manifest ...')\n",
    "    \n",
    "    metadata = pd.read_csv('METADATA.tsv', sep='\\t')\n",
    "    merged = pd.merge(merged,metadata, on='sample-id')\n",
    "    merged = merged.replace({'V2': '16S'}, regex=True)\n",
    "    print('Set up metadata ...')\n",
    "    \n",
    "    merged.to_csv(comm+'/merged_asvs_metadata.tsv', sep = '\\t')\n",
    "    print('Saved merged_asvs_metadata.tsv')\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up manifest ...\n",
      "Set up metadata ...\n",
      "Saved merged_asvs_metadata.tsv\n"
     ]
    }
   ],
   "source": [
    "merged = merge_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__rename_move_taxonomy__ rename taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_move_all_taxonomies():\n",
    "    dr = comm+'/all_trims'\n",
    "    if not os.path.isdir(comm+'/all_taxonomies'):\n",
    "        for root, dirs, files in os.walk(dr): #rename all taxonomy.tsv by their trimlengths\n",
    "            for file in files:\n",
    "                if file == \"taxonomy.tsv\":\n",
    "                    spl = root.split(\"/\"); newname = spl[-6]; sup = (\"/\").join(spl[:-6])\n",
    "                    shutil.copy(root+\"/\"+file, sup+\"/\"+newname+\".tsv\");# shutil.rmtree(root)\n",
    "        files = glob.glob('{0}F*R*.tsv'.format(comm+'/all_trims/'))\n",
    "        os.mkdir(comm+'/all_taxonomies')\n",
    "        for file in files:\n",
    "            shutil.move(file, comm+'/all_taxonomies/') #puts all tsvs in new directory with correct names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_move_all_taxonomies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__pick_metadata__ extracts the features according to the given metadata parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_metadata(composition, runnumber, R='all', F='all'):\n",
    "#make df of features/composition+run+comm\n",
    "\n",
    "    composition = composition\n",
    "    runnumber = runnumber\n",
    "    R = R\n",
    "    F = F\n",
    "\n",
    "    files = glob.glob('{0}*.tsv'.format(comm+'/all_taxonomies/'))\n",
    "    taxos = []\n",
    "#    if not os.path.exists(path+composition):\n",
    "#        os.mkdir(path+composition)\n",
    "    for filename in files:\n",
    "        tax = pd.read_csv(filename, sep='\\t')\n",
    "        tax['table_id'] = str(filename.split('/')[-1])\n",
    "        tax[\"table_id\"] = tax[\"table_id\"].str.replace(\".tsv\", \"\")\n",
    "        tax['Forward_trim'], tax['Reverse_trim'] = tax['table_id'].str.split('R', 1).str\n",
    "        tax['Forward_trim'] = tax['Forward_trim'].map(lambda x: x.lstrip('F'))\n",
    "        tax[\"Forward_trim\"] = pd.to_numeric(tax[\"Forward_trim\"])\n",
    "        tax[\"Reverse_trim\"] = pd.to_numeric(tax[\"Reverse_trim\"])\n",
    "        taxos.append(tax)\n",
    "    print('Appended all taxonomies to taxos')\n",
    "    taxos = pd.concat(taxos)\n",
    "    taxos = taxos.rename(columns={\"Feature ID\": \"feature_id\"}, errors=\"raise\")\n",
    "    taxos.to_csv(comm+'/taxos.tsv', sep = '\\t')\n",
    "    separated = merged.merge(taxos, how='left', on='feature_id')\n",
    "    separated = separated.drop_duplicates()\n",
    "    separated = separated[separated[\"community\"] == comm_id]\n",
    "    separated = separated[separated[\"composition\"] == composition]\n",
    "    separated['run-number']= separated['run-number'].astype(str)\n",
    "    separated = separated[separated[\"run-number\"] == runnumber]\n",
    "    separated['sum'] = separated.groupby(['table_id','sample-id'])['feature_frequency'].transform('sum')\n",
    "    separated['ratio'] = separated['feature_frequency']/(separated['sum'])\n",
    "    separated_taxonomies = separated.copy()\n",
    "    #make a dictionary with keys for id-ing the taxon belonging to this sub-community\n",
    "    separated_dic = pd.Series(separated.Taxon.values,separated.feature_id.values).to_dict()\n",
    "    \n",
    "    return composition, runnumber, R, F, separated_taxonomies, separated_dic, files, tax, separated, taxos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended all taxonomies to taxos\n"
     ]
    }
   ],
   "source": [
    "composition, runnumber, R, F, separated_taxonomies, separated_dic, files, tax, separated, taxos = pick_metadata('Staggered', '46')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__rename_taxonomies__ extracts zipped classification files from qiime2, renames them by the trimming length used, and moves them to a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_taxonomies():\n",
    "#generate folder of split taxonomies by runnumber and composition\n",
    "    # Directory\n",
    "    directory = composition+runnumber\n",
    "    # Parent Directory path\n",
    "    parent_dir = comm+'/all_taxonomies'\n",
    "    # Path\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    # Create the directory\n",
    "    # 'GeeksForGeeks' in\n",
    "    # '/home / User / Documents'\n",
    "    os.mkdir(path)\n",
    "    for filename in files:\n",
    "        taxonomy = pd.read_csv(filename, sep='\\t')\n",
    "        taxonomy = taxonomy.rename(columns={\"Feature ID\": \"feature_id\"}, errors=\"raise\")\n",
    "        newz = taxonomy.merge(merged, how='left', on='feature_id')\n",
    "        #new = newz.drop(['sample-id'], axis=1)\n",
    "        new = newz.drop_duplicates()\n",
    "        new = new[new[\"community\"] == comm_id]\n",
    "        new = new[new[\"composition\"] == composition]\n",
    "        new['run-number']= new['run-number'].astype(str)\n",
    "        new = new[new[\"run-number\"] == runnumber]\n",
    "        new = new[new.feature_frequency != 0]\n",
    "        new = new.rename(columns={\"feature_id\":\"Feature ID\"}, errors=\"raise\")\n",
    "        new = new[['Feature ID', 'Taxon', 'Confidence']].copy()\n",
    "        new = new.drop_duplicates()\n",
    "        d = 'all_taxonomies/'\n",
    "        new.to_csv(filename.split(d)[0]+d+composition+runnumber+'/'+runnumber+filename.split(d)[1], sep = '\\t') \n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature ID</th>\n",
       "      <th>Taxon</th>\n",
       "      <th>Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>863728e1cec6befd5ba02d15baef4c36</td>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>0.879390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>b8b0859fd6afcfc39c1a7e6a50415ca3</td>\n",
       "      <td>d__Bacteria; p__Proteobacteria; c__Alphaproteo...</td>\n",
       "      <td>0.988628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>bdf18e8980906003f3978e5ee578c9ea</td>\n",
       "      <td>d__Bacteria; p__Actinobacteriota; c__Acidimicr...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>b079d9bfa3d84b0bdc2c1398d247150e</td>\n",
       "      <td>d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...</td>\n",
       "      <td>0.986491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Feature ID  \\\n",
       "923   863728e1cec6befd5ba02d15baef4c36   \n",
       "1919  b8b0859fd6afcfc39c1a7e6a50415ca3   \n",
       "2892  bdf18e8980906003f3978e5ee578c9ea   \n",
       "3326  b079d9bfa3d84b0bdc2c1398d247150e   \n",
       "\n",
       "                                                  Taxon  Confidence  \n",
       "923   d__Bacteria; p__Proteobacteria; c__Alphaproteo...    0.879390  \n",
       "1919  d__Bacteria; p__Proteobacteria; c__Alphaproteo...    0.988628  \n",
       "2892  d__Bacteria; p__Actinobacteriota; c__Acidimicr...    1.000000  \n",
       "3326  d__Bacteria; p__Bacteroidota; c__Bacteroidia; ...    0.986491  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pick_taxonomies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__make_fasta__ extracts sequences from zipped qiime2 files, and makes new fasta file for the features from the given metadata parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fasta():\n",
    "    \n",
    "    fastaoutfilename = comm+'/all_seqs/allfasta.fasta'\n",
    "    \n",
    "    if not os.path.isfile(fastaoutfilename):\n",
    "        os.mkdir(comm+'/all_seqs')\n",
    "        # Get list of all .qza\n",
    "        repseqs = glob.glob('{0}/*/*/representative_sequences.qza'.format(comm+'/all_trims'), recursive=True)\n",
    "        for repseq in repseqs:\n",
    "            with zipfile.ZipFile(repseq, 'r') as zipObj:\n",
    "                # Get a list of all archived file names from the zip\n",
    "                listOfFileNames = zipObj.namelist()\n",
    "                # Iterate over the file names\n",
    "                for fileName in listOfFileNames:\n",
    "                    # Check filename endswith fasta\n",
    "                    if fileName.endswith('.fasta'):\n",
    "                        # Extract a single file from zip\n",
    "                        zipObj.extract(fileName, 'temp_fasta')\n",
    "                        \n",
    "    \n",
    "        with open(fastaoutfilename, 'wb') as outfile:\n",
    "            for filename in glob.glob('temp_fasta/*/*/*.fasta'):\n",
    "                if filename == fastaoutfilename:\n",
    "                    # don't want to copy the output into the output\n",
    "                    continue\n",
    "                with open(filename, 'rb') as readfile:\n",
    "                    shutil.copyfileobj(readfile, outfile)\n",
    "        shutil.rmtree('temp_fasta', ignore_errors=False, onerror=None)\n",
    "    \n",
    "\n",
    "    if R!='all':\n",
    "        rallfs = separated[separated.Reverse_trim == R]\n",
    "        separated_dic = pd.Series(rallfs.Taxon.values,rallfs.feature_id.values).to_dict()\n",
    "    else:\n",
    "        separated_dic = pd.Series(separated.Taxon.values, separated.feature_id.values).to_dict()\n",
    "    if F!='all':\n",
    "        fallrs = separated[separated.Forward_trim == F]\n",
    "        separated_dic = pd.Series(fallrs.Taxon.values,fallrs.feature_id.values).to_dict()\n",
    "    else:\n",
    "        separated_dic = pd.Series(separated.Taxon.values, separated.feature_id.values).to_dict()\n",
    "\n",
    "    fa = SeqIO.parse(comm+'/all_seqs/allfasta.fasta',\n",
    "                 \"fasta\")\n",
    "    seqs_i_want = [] #we'll put the good sequences here\n",
    "    for record in fa: #a SeqRecord has the accession as record.id, usually.\n",
    "        if record.id in separated_dic.keys(): #This is how you check if the accession is in the values of the dict\n",
    "            seqs_i_want.append(record)\n",
    "    #Now we can write the list of records to a fasta file. This will take care of the formatting etc\n",
    "    with open(comm+'/all_seqs/R'+R+'F'+F+runnumber+composition+'.fasta', \"w\") as f:\n",
    "        SeqIO.write(seqs_i_want, f, \"fasta\")\n",
    "    \n",
    "    return print('Saved selected sequences as '+comm+'/all_seqs/R'+R+'F'+F+runnumber+composition+'.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved selected sequences as 02-PROKs/all_seqs/RallFall46Staggered.fasta\n"
     ]
    }
   ],
   "source": [
    "make_fasta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__make_tbd_hm__ makes a heatmap showing the taxonomic beta diversity of each trim length combination against the expected community. TBD is a dissimilarity index based on taxonomic trees between two samples where 1 is completely different, and 0 is the completely the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tbd_hm(level=7):\n",
    "    \n",
    "    if not os.path.isdir('Bacaros_Beta'):\n",
    "        ! git clone https://github.com/alexmanuele/Bacaros_Beta.git\n",
    "    \n",
    "    #Move the expected community to the taxonomies folder for comparison\n",
    "    dest_dir = comm+'/all_taxonomies/'+composition+runnumber\n",
    "    \n",
    "    for file in glob.glob('in-silico-mocks/'+comm+'/'+composition+r'/*/data/taxonomy.tsv'):\n",
    "        print(file)\n",
    "        shutil.copy(file, os.path.join(dest_dir, 'expected.tsv'))\n",
    "    \n",
    "    tax_list = glob.glob('{0}/*.tsv'.format(comm+'/all_taxonomies/'+composition+runnumber))\n",
    "    textfile = open(\"tax_comp.txt\", \"w\")\n",
    "    for element in tax_list:\n",
    "        textfile.write(element + \"\\n\")\n",
    "    textfile.close()\n",
    "    print(\"Saved all taxonomies list as tax_comp.txt\")\n",
    "\n",
    "    if not os.path.isdir(dest_dir+'/TBD1'):\n",
    "        for i in range(1, 8):\n",
    "            output_dir = 'TBD'+str(i)\n",
    "            os.mkdir(output_dir)\n",
    "            ! python Bacaros_Beta/run_beta.py --input tax_comp.txt --metric t --l $i --output $output_dir\n",
    "            shutil.move(output_dir, dest_dir)\n",
    "        \n",
    "    bacaros_dm = pd.read_csv(dest_dir+'/TBD'+str(level)+'/tax_comp.csv')\n",
    "    bacaros_dm = bacaros_dm.set_index('Unnamed: 0')\n",
    "    bacaros_dm = 1  - bacaros_dm\n",
    "    #bacaros_dm is a distance matrix of table X table\n",
    "    my_pcoa = skbio.stats.ordination.pcoa(bacaros_dm.values)\n",
    "    plt.scatter(my_pcoa.samples['PC1'],  my_pcoa.samples['PC2'])\n",
    "    against_exp = bacaros_dm[['expected']].copy()\n",
    "    against_exp = against_exp.reset_index().rename(columns={against_exp.index.name:'sample_name'})\n",
    "    against_exp.drop(against_exp.index[against_exp['sample_name'] == 'expected'], inplace=True)\n",
    "    against_exp['Forward_trim'] = [s.split('R')[0] for s in against_exp['sample_name']]\n",
    "    against_exp['Forward_trim'] = [s.split('46F')[1] for s in against_exp['Forward_trim']]\n",
    "    against_exp['Reverse_trim'] = [s.split('R')[1] for s in against_exp['sample_name']]\n",
    "    against_exp[\"Forward_trim\"] = pd.to_numeric(against_exp[\"Forward_trim\"])\n",
    "    against_exp[\"Reverse_trim\"] = pd.to_numeric(against_exp[\"Reverse_trim\"])\n",
    "    against_exp[\"Forward_trim\"].replace({0: 280}, inplace=True)\n",
    "    against_exp[\"Reverse_trim\"].replace({0: 290}, inplace=True)\n",
    "    tohm = against_exp.pivot(\"Forward_trim\", \"Reverse_trim\", \"expected\")\n",
    "    tohm.rename({280: 'full'}, axis=0, inplace=True)\n",
    "    tohm.rename({290: 'full'}, axis=1, inplace=True)\n",
    "    ax = sns.heatmap(tohm, cmap=sns.color_palette(\"crest\"), vmin=0, vmax=1)\n",
    "    ax.invert_yaxis()\n",
    "    plt.figure(figsize=(12,12))\n",
    "    \n",
    "    # get max and min values\n",
    "    #print('The min value is {0} and the max value is {1}'.format(df.at[df.stack().index[np.argmin(df.values)]], \n",
    "    #                                                             minv = df.at[df.stack().index[np.argmax(df.values)]]))\n",
    "\n",
    "    return (tohm, bacaros_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in-silico-mocks/02-PROKs/Staggered/7fee7113-8e8a-459b-ae32-6d8381294d5c/data/taxonomy.tsv\n",
      "Saved all taxonomies list as tax_comp.txt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 0\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"Bacaros_Beta/run_beta.py\", line 33, in <module>\n",
      "    deltas, b = beta.calculate_beta(samples, L, metric)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 114, in calculate_beta\n",
      "    delta = dfunc(pair[0]['taxa'], pair[1]['taxa'], L)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 90, in delta_T\n",
      "    f2_nodes= frame2[col].dropna().unique()\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/frame.py\", line 2906, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 0\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"Bacaros_Beta/run_beta.py\", line 33, in <module>\n",
      "    deltas, b = beta.calculate_beta(samples, L, metric)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 114, in calculate_beta\n",
      "    delta = dfunc(pair[0]['taxa'], pair[1]['taxa'], L)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 90, in delta_T\n",
      "    f2_nodes= frame2[col].dropna().unique()\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/frame.py\", line 2906, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 0\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"Bacaros_Beta/run_beta.py\", line 33, in <module>\n",
      "    deltas, b = beta.calculate_beta(samples, L, metric)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 114, in calculate_beta\n",
      "    delta = dfunc(pair[0]['taxa'], pair[1]['taxa'], L)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 90, in delta_T\n",
      "    f2_nodes= frame2[col].dropna().unique()\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/frame.py\", line 2906, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 0\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"Bacaros_Beta/run_beta.py\", line 33, in <module>\n",
      "    deltas, b = beta.calculate_beta(samples, L, metric)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 114, in calculate_beta\n",
      "    delta = dfunc(pair[0]['taxa'], pair[1]['taxa'], L)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 90, in delta_T\n",
      "    f2_nodes= frame2[col].dropna().unique()\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/frame.py\", line 2906, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 0\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"Bacaros_Beta/run_beta.py\", line 33, in <module>\n",
      "    deltas, b = beta.calculate_beta(samples, L, metric)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 114, in calculate_beta\n",
      "    delta = dfunc(pair[0]['taxa'], pair[1]['taxa'], L)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 90, in delta_T\n",
      "    f2_nodes= frame2[col].dropna().unique()\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/frame.py\", line 2906, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 0\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 0\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"Bacaros_Beta/run_beta.py\", line 33, in <module>\n",
      "    deltas, b = beta.calculate_beta(samples, L, metric)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 114, in calculate_beta\n",
      "    delta = dfunc(pair[0]['taxa'], pair[1]['taxa'], L)\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 90, in delta_T\n",
      "    f2_nodes= frame2[col].dropna().unique()\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/frame.py\", line 2906, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2898, in get_loc\r\n",
      "    return self._engine.get_loc(casted_key)\r\n",
      "  File \"pandas/_libs/index.pyx\", line 70, in pandas._libs.index.IndexEngine.get_loc\r\n",
      "  File \"pandas/_libs/index.pyx\", line 101, in pandas._libs.index.IndexEngine.get_loc\r\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1675, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1683, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\n",
      "KeyError: 0\r\n",
      "\r\n",
      "The above exception was the direct cause of the following exception:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"Bacaros_Beta/run_beta.py\", line 33, in <module>\r\n",
      "    deltas, b = beta.calculate_beta(samples, L, metric)\r\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 114, in calculate_beta\r\n",
      "    delta = dfunc(pair[0]['taxa'], pair[1]['taxa'], L)\r\n",
      "  File \"/Users/Diana/MOCK_ANALYSIS/Bacaros_Beta/beta/beta.py\", line 90, in delta_T\r\n",
      "    f2_nodes= frame2[col].dropna().unique()\r\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/frame.py\", line 2906, in __getitem__\r\n",
      "    indexer = self.columns.get_loc(key)\r\n",
      "  File \"/Users/Diana/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/core/indexes/base.py\", line 2900, in get_loc\r\n",
      "    raise KeyError(key) from err\r\n",
      "KeyError: 0\r\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '02-PROKs/all_taxonomies/Staggered46/TBD7/tax_comp.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-3fb6b29cc474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmake_tbd_hm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-174-f04ede5525bf>\u001b[0m in \u001b[0;36mmake_tbd_hm\u001b[0;34m(level)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mbacaros_dm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/TBD'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/tax_comp.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mbacaros_dm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacaros_dm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbacaros_dm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0mbacaros_dm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/qiime2-2020.111/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '02-PROKs/all_taxonomies/Staggered46/TBD7/tax_comp.csv'"
     ]
    }
   ],
   "source": [
    "make_tbd_hm(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__r2_plot__ makes a linear regression of the observed relative abundances of each combination of trim lengths against the expected, and plots each coefficient of determination in a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to import the expected taxonomies and transofrm to ratios\n",
    "def r2_plot():\n",
    "    expected_silva = pd.read_csv(dest_dir+'/expected.tsv', sep='\\t')\n",
    "    expected_gg = pd.read_csv('in-silico-mocks/expected_all.tsv')\n",
    "    expected = expected.rename(columns={'silva_taxonomy':'Taxon', 'sample-id': 'Replicate'})\n",
    "    expected_even = expected[expected.mock_even_insilico != 0]\n",
    "    expected_even = expected_even.drop(columns=['mock_staggered_insilico','taxonomy'])\n",
    "    expected_even.reset_index(drop=True, inplace=True)\n",
    "    expected_even['expected_ratio'] = expected_even['mock_even_insilico']/(expected_even['mock_even_insilico'].sum())\n",
    "    expected_stagg = expected.drop(columns=['mock_even_insilico','taxonomy'])\n",
    "    expected_stagg.reset_index(drop=True, inplace=True)\n",
    "    expected_stagg['expected_ratio'] = expected_stagg['mock_staggered_insilico']/(expected_stagg['mock_staggered_insilico'].sum())\n",
    "    \n",
    "    return (expected_even, expected_stagg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_silva = pd.read_csv(dest_dir+'/expected.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftables = glob.glob('{0}/dereplicated-table.qza'.format('in-silico-mocks/'+comm+'/'+composition), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in-silico-mocks/02-PROKs/Staggered/dereplicated-table.qza']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftoutname = 'in-silico-mocks/'+comm+'/'+composition+'/feature_table.biom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ftable in ftables:\n",
    "    with zipfile.ZipFile(ftable, 'r') as zipObj:\n",
    "        # Get a list of all archived file names from the zip\n",
    "        listOfFileNames = zipObj.namelist()\n",
    "        # Iterate over the file names\n",
    "        for fileName in listOfFileNames:\n",
    "            # Check filename endswith fasta\n",
    "            if fileName.endswith('.biom'):\n",
    "                # Extract a single file from zip\n",
    "                zipObj.extract(fileName, 'temp_biom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ftoutname, 'wb') as outfile:\n",
    "    for filename in glob.glob('temp_biom/*/*/*.biom'):\n",
    "        if filename == ftoutname:\n",
    "            # don't want to copy the output into the output\n",
    "            continue\n",
    "            with open(filename, 'rb') as readfile:\n",
    "                shutil.copyfileobj(readfile, outfile)\n",
    "            shutil.rmtree('temp_fasta', ignore_errors=False, onerror=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_gg = pd.read_csv('in-silico-mocks/expected_all.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_gg = expected_gg.replace(0,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_gg_16S_Staggered = expected_gg.loc[expected_gg['16S_Staggered'].notnull(), ['#OTU ID', '16S_Staggered', 'taxonomy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#OTU ID</th>\n",
       "      <th>16S_Staggered</th>\n",
       "      <th>taxonomy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>863728e1cec6befd5ba02d15baef4c36</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__SAR11 clade; D_4__Clade I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bdf18e8980906003f3978e5ee578c9ea</td>\n",
       "      <td>700.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Actinobacteria; D_2__Acidimicrobiia; D_3__Actinomarinales; D_4__Actinomarinaceae; D_5__Candidatus Actinomarina; D_6__uncultured marine bacterium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2970af3e30642198ba1dfa1f304f310b</td>\n",
       "      <td>400.0</td>\n",
       "      <td>D_0__Archaea; D_1__Thaumarchaeota; D_2__Nitrososphaeria; D_3__Nitrosopumilales; D_4__Nitrosopumilaceae; D_5__Candidatus Nitrosopumilus; Ambiguous_taxa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>beb8cb17f8152e488b6bc95bfcdb484f</td>\n",
       "      <td>400.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Actinobacteria; D_2__Acidimicrobiia; D_3__Actinomarinales; D_4__Actinomarinaceae; D_5__Candidatus Actinomarina; Ambiguous_taxa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>86cd70d2450e3f44f4a7c543f53008ee</td>\n",
       "      <td>300.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Cyanobacteria; D_2__Oxyphotobacteria; D_3__Synechococcales; D_4__Cyanobiaceae; D_5__Prochlorococcus MIT9313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>f4ad315ec51d8bee23cb39f39bc8b019</td>\n",
       "      <td>200.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__SAR86 clade; D_4__Rhodobacteraceae bacterium REDSEA-S29_B10; D_5__Rhodobacteraceae bacterium REDSEA-S29_B10; D_6__Rhodobacteraceae bacterium REDSEA-S29_B10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>b8b0859fd6afcfc39c1a7e6a50415ca3</td>\n",
       "      <td>100.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__Puniceispirillales; D_4__SAR116 clade; D_5__uncultured marine bacterium; D_6__uncultured marine bacterium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0b1c2b9ed02095f223b442c9bdd35006</td>\n",
       "      <td>100.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__Rhodospirillales; D_4__AEGEAN-169 marine group; D_5__uncultured marine bacterium; D_6__uncultured marine bacterium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0b7809b1b04598cfc5bbf6264159dde9</td>\n",
       "      <td>80.0</td>\n",
       "      <td>D_0__Archaea; D_1__Euryarchaeota; D_2__Thermoplasmata; D_3__Marine Group II</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20a0d2f150b3312772f5c00e54fd7baf</td>\n",
       "      <td>80.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Bacteroidetes; D_2__Bacteroidia; D_3__Flavobacteriales; D_4__Flavobacteriaceae; D_5__NS2b marine group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32debaa2edfdd6b9fcefff9d65ff740f</td>\n",
       "      <td>80.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Chloroflexi; D_2__Dehalococcoidia; D_3__SAR202 clade; Ambiguous_taxa; Ambiguous_taxa; Ambiguous_taxa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9c0fe787b120b62b93c3cb46d844d9fa</td>\n",
       "      <td>80.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Planctomycetes; D_2__Planctomycetacia; D_3__Pirellulales; D_4__Pirellulaceae; D_5__Blastopirellula; D_6__uncultured Pirellula sp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ddeb7a85942593ec57e0cd5f3e06701e</td>\n",
       "      <td>80.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__Puniceispirillales; D_4__SAR116 clade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>167b5b3eeed0f9b502f7f1f245d5116c</td>\n",
       "      <td>60.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Marinimicrobia (SAR406 clade); D_2__uncultured marine bacterium; D_3__uncultured marine bacterium; D_4__uncultured marine bacterium; D_5__uncultured marine bacterium; D_6__uncultured marine bacterium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>339277019fe12bc3cac9b8895e27d5cc</td>\n",
       "      <td>40.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Verrucomicrobia; D_2__Verrucomicrobiae; D_3__Opitutales; D_4__Puniceicoccaceae; D_5__MB11C04 marine group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>843d32e6b6c1caf98f73a506a167e307</td>\n",
       "      <td>40.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__SAR86 clade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>934545da41a2aaae09253aec77b18cbd</td>\n",
       "      <td>40.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__Oceanospirillales; D_4__Pseudohongiellaceae; D_5__Pseudohongiella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>af221dfa5b6c65292b9eaacdbeae6b0e</td>\n",
       "      <td>40.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__Cellvibrionales; D_4__Porticoccaceae; D_5__SAR92 clade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>b079d9bfa3d84b0bdc2c1398d247150e</td>\n",
       "      <td>40.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Bacteroidetes; D_2__Bacteroidia; D_3__Flavobacteriales; D_4__Flavobacteriaceae; D_5__Formosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>c97ab403dfec27de969d1b6debc9e4fb</td>\n",
       "      <td>40.0</td>\n",
       "      <td>D_0__Archaea; D_1__Thaumarchaeota; D_2__Nitrososphaeria; D_3__Nitrosopumilales; D_4__Nitrosopumilaceae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>e7e4bfdc8dfad206c1f1638a29b2f703</td>\n",
       "      <td>40.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Bacteroidetes; D_2__Bacteroidia; D_3__Flavobacteriales; D_4__NS9 marine group; Ambiguous_taxa; Ambiguous_taxa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>73748067baf6a8d8dfd693497d93c01f</td>\n",
       "      <td>30.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__Rhodobacterales; D_4__Rhodobacteraceae; D_5__uncultured</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>cc1e9cc7b5d5c955b025d6e85c5bab2d</td>\n",
       "      <td>30.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__SAR86 clade; Ambiguous_taxa; Ambiguous_taxa; Ambiguous_taxa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>06ad8aee2710d4b77d4121a90e32bfb7</td>\n",
       "      <td>20.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Bacteroidetes; D_2__Bacteroidia; D_3__Flavobacteriales; D_4__Flavobacteriaceae; D_5__NS5 marine group; D_6__uncultured marine bacterium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>a2c45679733004eb45e94334e3d5094f</td>\n",
       "      <td>10.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__SAR86 clade; Ambiguous_taxa; Ambiguous_taxa; Ambiguous_taxa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3216b39d4af1de8cb3faee0adae93a2b</td>\n",
       "      <td>5.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__Puniceispirillales; D_4__SAR116 clade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>86440f2600e40a23725d58e958687843</td>\n",
       "      <td>5.0</td>\n",
       "      <td>D_0__Bacteria; D_1__Chloroflexi; D_2__Dehalococcoidia; D_3__SAR202 clade; D_4__uncultured Chloroflexi bacterium; D_5__uncultured Chloroflexi bacterium; D_6__uncultured Chloroflexi bacterium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             #OTU ID  16S_Staggered  \\\n",
       "16  863728e1cec6befd5ba02d15baef4c36         1400.0   \n",
       "17  bdf18e8980906003f3978e5ee578c9ea          700.0   \n",
       "18  2970af3e30642198ba1dfa1f304f310b          400.0   \n",
       "19  beb8cb17f8152e488b6bc95bfcdb484f          400.0   \n",
       "20  86cd70d2450e3f44f4a7c543f53008ee          300.0   \n",
       "21  f4ad315ec51d8bee23cb39f39bc8b019          200.0   \n",
       "22  b8b0859fd6afcfc39c1a7e6a50415ca3          100.0   \n",
       "23  0b1c2b9ed02095f223b442c9bdd35006          100.0   \n",
       "24  0b7809b1b04598cfc5bbf6264159dde9           80.0   \n",
       "25  20a0d2f150b3312772f5c00e54fd7baf           80.0   \n",
       "26  32debaa2edfdd6b9fcefff9d65ff740f           80.0   \n",
       "27  9c0fe787b120b62b93c3cb46d844d9fa           80.0   \n",
       "28  ddeb7a85942593ec57e0cd5f3e06701e           80.0   \n",
       "29  167b5b3eeed0f9b502f7f1f245d5116c           60.0   \n",
       "30  339277019fe12bc3cac9b8895e27d5cc           40.0   \n",
       "31  843d32e6b6c1caf98f73a506a167e307           40.0   \n",
       "32  934545da41a2aaae09253aec77b18cbd           40.0   \n",
       "33  af221dfa5b6c65292b9eaacdbeae6b0e           40.0   \n",
       "34  b079d9bfa3d84b0bdc2c1398d247150e           40.0   \n",
       "35  c97ab403dfec27de969d1b6debc9e4fb           40.0   \n",
       "36  e7e4bfdc8dfad206c1f1638a29b2f703           40.0   \n",
       "37  73748067baf6a8d8dfd693497d93c01f           30.0   \n",
       "38  cc1e9cc7b5d5c955b025d6e85c5bab2d           30.0   \n",
       "39  06ad8aee2710d4b77d4121a90e32bfb7           20.0   \n",
       "40  a2c45679733004eb45e94334e3d5094f           10.0   \n",
       "41  3216b39d4af1de8cb3faee0adae93a2b            5.0   \n",
       "42  86440f2600e40a23725d58e958687843            5.0   \n",
       "\n",
       "                                                                                                                                                                                                                          taxonomy  \n",
       "16                                                                                                                                    D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__SAR11 clade; D_4__Clade I  \n",
       "17                                                            D_0__Bacteria; D_1__Actinobacteria; D_2__Acidimicrobiia; D_3__Actinomarinales; D_4__Actinomarinaceae; D_5__Candidatus Actinomarina; D_6__uncultured marine bacterium  \n",
       "18                                                                          D_0__Archaea; D_1__Thaumarchaeota; D_2__Nitrososphaeria; D_3__Nitrosopumilales; D_4__Nitrosopumilaceae; D_5__Candidatus Nitrosopumilus; Ambiguous_taxa  \n",
       "19                                                                              D_0__Bacteria; D_1__Actinobacteria; D_2__Acidimicrobiia; D_3__Actinomarinales; D_4__Actinomarinaceae; D_5__Candidatus Actinomarina; Ambiguous_taxa  \n",
       "20                                                                                                 D_0__Bacteria; D_1__Cyanobacteria; D_2__Oxyphotobacteria; D_3__Synechococcales; D_4__Cyanobiaceae; D_5__Prochlorococcus MIT9313  \n",
       "21  D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__SAR86 clade; D_4__Rhodobacteraceae bacterium REDSEA-S29_B10; D_5__Rhodobacteraceae bacterium REDSEA-S29_B10; D_6__Rhodobacteraceae bacterium REDSEA-S29_B10  \n",
       "22                                                    D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__Puniceispirillales; D_4__SAR116 clade; D_5__uncultured marine bacterium; D_6__uncultured marine bacterium  \n",
       "23                                           D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__Rhodospirillales; D_4__AEGEAN-169 marine group; D_5__uncultured marine bacterium; D_6__uncultured marine bacterium  \n",
       "24                                                                                                                                                     D_0__Archaea; D_1__Euryarchaeota; D_2__Thermoplasmata; D_3__Marine Group II  \n",
       "25                                                                                                      D_0__Bacteria; D_1__Bacteroidetes; D_2__Bacteroidia; D_3__Flavobacteriales; D_4__Flavobacteriaceae; D_5__NS2b marine group  \n",
       "26                                                                                                        D_0__Bacteria; D_1__Chloroflexi; D_2__Dehalococcoidia; D_3__SAR202 clade; Ambiguous_taxa; Ambiguous_taxa; Ambiguous_taxa  \n",
       "27                                                                           D_0__Bacteria; D_1__Planctomycetes; D_2__Planctomycetacia; D_3__Pirellulales; D_4__Pirellulaceae; D_5__Blastopirellula; D_6__uncultured Pirellula sp.  \n",
       "28                                                                                                                        D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__Puniceispirillales; D_4__SAR116 clade  \n",
       "29     D_0__Bacteria; D_1__Marinimicrobia (SAR406 clade); D_2__uncultured marine bacterium; D_3__uncultured marine bacterium; D_4__uncultured marine bacterium; D_5__uncultured marine bacterium; D_6__uncultured marine bacterium  \n",
       "30                                                                                                   D_0__Bacteria; D_1__Verrucomicrobia; D_2__Verrucomicrobiae; D_3__Opitutales; D_4__Puniceicoccaceae; D_5__MB11C04 marine group  \n",
       "31                                                                                                                                                  D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__SAR86 clade  \n",
       "32                                                                                            D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__Oceanospirillales; D_4__Pseudohongiellaceae; D_5__Pseudohongiella  \n",
       "33                                                                                                       D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__Cellvibrionales; D_4__Porticoccaceae; D_5__SAR92 clade  \n",
       "34                                                                                                                D_0__Bacteria; D_1__Bacteroidetes; D_2__Bacteroidia; D_3__Flavobacteriales; D_4__Flavobacteriaceae; D_5__Formosa  \n",
       "35                                                                                                                          D_0__Archaea; D_1__Thaumarchaeota; D_2__Nitrososphaeria; D_3__Nitrosopumilales; D_4__Nitrosopumilaceae  \n",
       "36                                                                                               D_0__Bacteria; D_1__Bacteroidetes; D_2__Bacteroidia; D_3__Flavobacteriales; D_4__NS9 marine group; Ambiguous_taxa; Ambiguous_taxa  \n",
       "37                                                                                                      D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__Rhodobacterales; D_4__Rhodobacteraceae; D_5__uncultured  \n",
       "38                                                                                                  D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__SAR86 clade; Ambiguous_taxa; Ambiguous_taxa; Ambiguous_taxa  \n",
       "39                                                                     D_0__Bacteria; D_1__Bacteroidetes; D_2__Bacteroidia; D_3__Flavobacteriales; D_4__Flavobacteriaceae; D_5__NS5 marine group; D_6__uncultured marine bacterium  \n",
       "40                                                                                                  D_0__Bacteria; D_1__Proteobacteria; D_2__Gammaproteobacteria; D_3__SAR86 clade; Ambiguous_taxa; Ambiguous_taxa; Ambiguous_taxa  \n",
       "41                                                                                                                        D_0__Bacteria; D_1__Proteobacteria; D_2__Alphaproteobacteria; D_3__Puniceispirillales; D_4__SAR116 clade  \n",
       "42                                   D_0__Bacteria; D_1__Chloroflexi; D_2__Dehalococcoidia; D_3__SAR202 clade; D_4__uncultured Chloroflexi bacterium; D_5__uncultured Chloroflexi bacterium; D_6__uncultured Chloroflexi bacterium  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_gg_16S_Staggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = df.loc[df['Survive'].notnull(), ['Age','Fare', 'Group_Size','deck', 'Pclass', 'Title' ]]\n",
    "xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__get_fig_per_group__ makes a heatmap of the observed against expected relative abundances for given group names and colors ratios above the expected in blues, and below the expected in reds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fig_per_group(groupname, expectedratio, duplicate='mean'):\n",
    "    neoceratium = separated[separated['Taxon'].str.contains(groupname)]\n",
    "    neoceratium = neoceratium[neoceratium.feature_frequency !=0]\n",
    "    neoceratium.rename(columns = {'sample-id':'sample_id'}, inplace=True)\n",
    "    if duplicate!='mean':\n",
    "        neoceratiumR1 = neoceratium[neoceratium.sample_id == 'R46-18S-'+duplicate]\n",
    "    else:\n",
    "        neoceratiumR1 = neoceratium.groupby(['Forward_trim','Reverse_trim'])[['ratio']].mean()\n",
    "        neoceratiumR1 = neoceratiumR1.reset_index()\n",
    "    neoceratiumR1[\"Forward_trim\"] = pd.to_numeric(neoceratiumR1[\"Forward_trim\"])\n",
    "    neoceratiumR1[\"Reverse_trim\"] = pd.to_numeric(neoceratiumR1[\"Reverse_trim\"])\n",
    "    neoceratiumR1[\"Forward_trim\"].replace({0: 280}, inplace=True)\n",
    "    neoceratiumR1[\"Reverse_trim\"].replace({0: 290}, inplace=True)\n",
    "    neoceratiumR1merged = neoceratiumR1.groupby(['Forward_trim','Reverse_trim'])[['ratio']].mean()\n",
    "    neoceratiumR1merged = neoceratiumR1merged.reset_index()\n",
    "    tohm = neoceratiumR1merged.pivot(\"Forward_trim\", \"Reverse_trim\", \"ratio\")\n",
    "    tohm.rename({280: 'full'}, axis=0, inplace=True)\n",
    "    tohm.rename({290: 'full'}, axis=1, inplace=True)\n",
    "    ax = sns.heatmap(tohm, cmap=\"Oranges_r\")#, mask= (tohm < (expectedratio-(0.0005*expectedratio))) & (tohm > (expectedratio+(0.0005*expectedratio)))) #cmap=sns.color_palette(\"hls\", 90)\n",
    "    ax = sns.heatmap(tohm, mask=tohm <= expectedratio, cmap=sns.color_palette(\"GnBu\", 5)) #square=True, annot=False, vmin=0, vmax=1, cbar=False, ax=ax)\n",
    "    #ax = sns.heatmap(tohm, mask=tohm >= expectedratio, cmap=sns.color_palette(\"Oranges_r\", 5)) #square=True, annot=False, vmin=0, vmax=1, cbar=False, ax=ax)\n",
    "    ax.invert_yaxis()\n",
    "    plt.figure(figsize=(12,12))\n",
    "    ax.set(xlabel='Reverse trim length', ylabel='Forward trim length')\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig('ratio'+groupname+'.png', bbox_inches = \"tight\")\n",
    "    return neoceratiumR1merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ttst__ runs one sample t tests between each taxonomic groups observed against expected relative abundances, and plots the results in a boxplot for a single trimming combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run 1 sample t test\n",
    "#takes trimcombination in format F40R40\n",
    "def ttst(trimcombination):\n",
    "    compr_obs_exp = expstagg.merge(separated, how='outer', on='Taxon')\n",
    "    compr_obs_exp = compr_obs_exp.rename(columns={'sample-id': 'Replicate'})\n",
    "    compr_obs_exp=compr_obs_exp.fillna(0)\n",
    "    compr_obs_exp[\"group\"].replace({0: \"False positive\"}, inplace=True)\n",
    "    df_with_groups = compr_obs_exp[compr_obs_exp.table_id == trimcombination]\n",
    "    \n",
    "    \n",
    "    means = df_with_groups.groupby('group').mean()\n",
    "    newsi=means[['expected_ratio', 'ratio']].copy()\n",
    "    newsi.sort_values('expected_ratio', ascending = False)\n",
    "    newsi['Difference'] = newsi['expected_ratio'] / newsi['ratio']\n",
    "    newsi\n",
    "    \n",
    "    # generate a boxplot to see the data distribution by treatments. Using boxplot, we can easily detect the differences between different treatments\n",
    "    ax = sns.boxplot(x='ratio', y='group', data=df_with_groups).set(\n",
    "    xlabel='Relative abundance', \n",
    "    ylabel='Group'\n",
    "    )\n",
    "    #ax.tick_params(axis='x', labelrotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "    results = []\n",
    "    for group in groups:    \n",
    "        arr = df_with_groups[(df_with_groups['group'] == group)]['ratio'].values  #Filter the dataframe. \n",
    "        results.append({'group': group,\n",
    "                        'ratios': arr}) #Make a single \"record\" containing the table id, replicate, and ratio array.\n",
    "        r_gr = pd.DataFrame.from_records(results)\n",
    "    \n",
    "    y = expstagg['expected_ratio']\n",
    "    for i in range(len(y[0:-1])):\n",
    "        xi = r_gr.iloc[i]['ratios']\n",
    "        yi = y[i]\n",
    "        print(r_gr.iloc[i]['group'])\n",
    "        print(stats.ttest_1samp(xi, yi))\n",
    "        #but there are outliers\n",
    "    return (r_gr, newsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run analyses and make figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_figures(community, composition, runnumber, R='all', F='all',level=7):\n",
    "    df, comm = consolidate_tables(community) #concat all feature tables and their pipeline parameters\n",
    "    merged = merge_metada() #add the metadata\n",
    "    separated_taxonomies, separated_dic = pick_metadata(composition, runnumber, R='all', F='all') #extract the features only from one community\n",
    "    rename_taxonomies() #rename the taxonomy tsv files for taxonomic beta diversity\n",
    "    make_fasta() #extract the sequences\n",
    "    make_tbd_hm(level=7) #make TBD heatmap\n",
    "    ## add function to find best trim length ranges (white box)\n",
    "    ## add table of thresholds + line plot\n",
    "    #r2_plot()\n",
    "    ##run t tests for best TBD and best r2, make boxplot\n",
    "    #get_fig_per_group()\n",
    "    ## get sequence comparison\n",
    "    ## get files for import to evolview --tree, abundance table, branch coloring and expected ratios\n",
    "    \n",
    "    return figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_figures('18S', 'Even', '46')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qiime2-2020.111",
   "language": "python",
   "name": "qiime2-2020.111"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
