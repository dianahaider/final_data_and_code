{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of varying trimming thresholds of microbial communities of known composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script for the analysis and figure generation o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries in python3 kernel\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import boto\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import skbio\n",
    "#import fastcluster #this package makes skbio run faster clustermaps but can be tricky with missing values from pairwise comparisons\n",
    "from functools import reduce\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "from Bio.SeqUtils import GC\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import statistics\n",
    "import itertools as it\n",
    "from scipy import stats\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from qiime2 import Artifact\n",
    "import tempfile\n",
    "import zipfile\n",
    "import yaml\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__consolidate_tables__ creates a dataframe of all the merged feature tables and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special thanks to Alex Manuele https://github.com/alexmanuele\n",
    "def consolidate_tables(community):\n",
    "    if community == \"16S\":\n",
    "        comm = '02-PROKs'\n",
    "    if community == \"18S\":\n",
    "        comm = '02-EUKs'\n",
    "\n",
    "\n",
    "    table_list = glob.glob('{0}*/03-DADA2d/table.qza'.format(comm+'/all_trims/'))\n",
    "    print \"Found all tables\"\n",
    "\n",
    "    dataframes = []  \n",
    "    for table_path in table_list:\n",
    "        with tempfile.TemporaryDirectory() as tempdir:\n",
    "            #load table, dump contents to tempdir\n",
    "            table = Artifact.load(table_path)\n",
    "            #Make sure the tables are all FeatureFrequency type\n",
    "            assert str(table.type) == 'FeatureTable[Frequency]', \"{0}: Expected FeatureTable[Frequency], got {1}\".format(table_path, table.type)\n",
    "            Artifact.extract(table_path, tempdir)\n",
    "            #get the provenance form the tempdir and format it for DF\n",
    "            prov = '{0}/{1}/provenance/'.format(tempdir, table.uuid)\n",
    "            action = yaml.load(open(\"{0}action/action.yaml\".format(prov), 'r'), Loader=yaml.BaseLoader)\n",
    "            paramlist = action['action']['parameters']\n",
    "            paramlist.append({'table_uuid': \"{}\".format(table.uuid)})\n",
    "            paramdict = {}\n",
    "            for record in paramlist:\n",
    "                paramdict.update(record)\n",
    "\n",
    "            # Get the data into a dataframe\n",
    "              #Biom data\n",
    "            df = table.view(pd.DataFrame).unstack().reset_index()\n",
    "            df.columns = ['feature_id', 'sample_name', 'feature_frequency']\n",
    "            df['table_uuid'] = [\"{}\".format(table.uuid)] * df.shape[0]\n",
    "              #param data\n",
    "            pdf = pd.DataFrame.from_records([paramdict])\n",
    "              #merge params into main df\n",
    "            df = df.merge(pdf, on='table_uuid')\n",
    "\n",
    "\n",
    "            #I like having these columns as the last three. Makes it more readable\n",
    "            cols = df.columns.tolist()\n",
    "            reorder = ['sample_name', 'feature_id', 'feature_frequency']\n",
    "            for val in reorder:\n",
    "                cols.append(cols.pop(cols.index(val)))\n",
    "            df = df[cols]\n",
    "            df['table_path'] = [table_path] * df.shape[0]\n",
    "            dataframes.append(df)\n",
    "\n",
    "    #Stick all the dataframes together\n",
    "    outputfile=\"merged_all_tables.tsv\"\n",
    "    df = pd.concat(dataframes)\n",
    "    df.to_csv(comm+outputfile, sep='\\t', index=False)\n",
    "    print(\"Success.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__merge_metadata__ adds the metadata to the merged feature tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_metadata():\n",
    "    #df = pd.read_csv('02-PROKs/'+'/merged_all_tables.tsv', sep='\\t')\n",
    "\n",
    "    tables = df[['sample_name', 'feature_id', 'feature_frequency']].copy()\n",
    "    tables.rename(columns={'sample_name':'file'}, inplace=True)\n",
    "    manifest = pd.read_csv('MANIFEST.tsv', sep='\\t')\n",
    "    manifest['file'] = [s.split('SPOT_USC_2/')[1] for s in manifest['absolute-filepath']]\n",
    "    manifest['file'] = [s.split('.R')[0] for s in manifest['file']]\n",
    "    manifest = manifest.drop(columns = ['absolute-filepath', 'direction'])\n",
    "    manifest.drop_duplicates()\n",
    "    merged = pd.merge(tables,manifest, on='file')\n",
    "    merged = merged.drop(columns = ['file'])\n",
    "    merged = merged.drop_duplicates() \n",
    "    print('Set up manifest ...')\n",
    "    \n",
    "    metadata = pd.read_csv('METADATA.tsv', sep='\\t')\n",
    "    merged = pd.merge(merged,metadata, on='sample-id')\n",
    "    merged = merged.replace({'V2': '16S'}, regex=True)\n",
    "    print('Set up metadata ...')\n",
    "    \n",
    "    merged.to_csv(comm+'filtering_asvs.tsv', sep = '\\t')\n",
    "    print('Saved filtering_asvs.tsv')\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__pick_metadata__ extracts the features according to the given metadata parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_metadata(composition, runnumber, R='all', F='all')\n",
    "#make df of features/composition+run+comm\n",
    "\n",
    "    composition = composition\n",
    "    runnumber = runnumber\n",
    "    R = R\n",
    "    F = F\n",
    "\n",
    "    files = glob.glob('{0}*.tsv'.format(comm+'/all_taxonomies_'+community+'/'))\n",
    "    taxos = []\n",
    "#    if not os.path.exists(path+composition):\n",
    "#        os.mkdir(path+composition)\n",
    "    for filename in files:\n",
    "        tax = pd.read_csv(filename, sep='\\t')\n",
    "        tax['table_id'] = str(filename.split('/')[-1])\n",
    "        tax[\"table_id\"] = tax[\"table_id\"].str.replace(\".tsv\", \"\")\n",
    "        tax['Forward_trim'], tax['Reverse_trim'] = tax['table_id'].str.split('R', 1).str\n",
    "        tax['Forward_trim'] = tax['Forward_trim'].map(lambda x: x.lstrip('F'))\n",
    "        tax[\"Forward_trim\"] = pd.to_numeric(tax[\"Forward_trim\"])\n",
    "        tax[\"Reverse_trim\"] = pd.to_numeric(tax[\"Reverse_trim\"])\n",
    "        taxos.append(tax)\n",
    "    print('Appended all taxonomies to taxos')\n",
    "    taxos = pd.concat(taxos)\n",
    "    taxos = taxos.rename(columns={\"Feature ID\": \"feature_id\"}, errors=\"raise\")\n",
    "    separated = taxos.merge(merged, how='left', on='feature_id')\n",
    "    separated = separated.drop_duplicates()\n",
    "    separated = separated[separated[\"community\"] == community]\n",
    "    separated = separated[separated[\"composition\"] == composition]\n",
    "    separated['run-number']= separated['run-number'].astype(str)\n",
    "    separated = separated[separated[\"run-number\"] == runnumber]\n",
    "    separated['sum'] = separated.groupby(['table_id','sample-id'])['feature_frequency'].transform('sum')\n",
    "    separated['ratio'] = separated['feature_frequency']/(separated['sum'])\n",
    "    separated_taxonomies = separated.copy()\n",
    "    #make a dictionary with keys for id-ing the taxon belonging to this sub-community\n",
    "    separated_dic = pd.Series(separated.Taxon.values,separated.feature_id.values).to_dict()\n",
    "    \n",
    "    return separated_taxonomies, separated_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__rename_taxonomies__ extracts zipped classification files from qiime2, renames them by the trimming length used, and moves them to a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_taxonomies()\n",
    "#generate folder of split taxonomies by runnumber and composition\n",
    "    # Directory\n",
    "    directory = composition+runnumber\n",
    "    # Parent Directory path\n",
    "    parent_dir = 'input_data/'+community+'/all_taxonomies_'+community\n",
    "    # Path\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    # Create the directory\n",
    "    # 'GeeksForGeeks' in\n",
    "    # '/home / User / Documents'\n",
    "    os.mkdir(path)\n",
    "    for filename in files:\n",
    "        taxonomy = pd.read_csv(filename, sep='\\t')\n",
    "        taxonomy = taxonomy.rename(columns={\"Feature ID\": \"feature_id\"}, errors=\"raise\")\n",
    "        newz = taxonomy.merge(merged, how='left', on='feature_id')\n",
    "        #new = newz.drop(['sample-id'], axis=1)\n",
    "        new = newz.drop_duplicates()\n",
    "        new = new[new[\"community\"] == community]\n",
    "        new = new[new[\"composition\"] == composition]\n",
    "        new['run-number']= new['run-number'].astype(str)\n",
    "        new = new[new[\"run-number\"] == runnumber]\n",
    "        new = new[new.feature_frequency != 0]\n",
    "        new = new.rename(columns={\"feature_id\":\"Feature ID\"}, errors=\"raise\")\n",
    "        new = new[['Feature ID', 'Taxon', 'Confidence']].copy()\n",
    "        new = new.drop_duplicates()\n",
    "        new.to_csv(filename.split('all_taxonomies_'+community)[0]+'all_taxonomies_'+community+'/'+composition+runnumber+'/'+runnumber+filename.split('all_taxonomies_'+community+'/')[1], sep = '\\t') \n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__make_fasta__ extracts sequences from zipped qiime2 files, and makes new fasta file for the features from the given metadata parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fasta()\n",
    "# Get list of all .qza\n",
    "    repseqs = glob.glob('{0}*/*/representative_sequences.qza'.format('/Users/Diana/Documents/escuela/phd/plugin_paper/mock_code/18S/all_trims/'), recursive=True)\n",
    "    for repseq in repseqs:\n",
    "        with ZipFile(repseq, 'r') as zipObj:\n",
    "            # Get a list of all archived file names from the zip\n",
    "            listOfFileNames = zipObj.namelist()\n",
    "            # Iterate over the file names\n",
    "            for fileName in listOfFileNames:\n",
    "                # Check filename endswith fasta\n",
    "                if fileName.endswith('.fasta'):\n",
    "                    # Extract a single file from zip\n",
    "                    zipObj.extract(fileName, 'temp_fasta')\n",
    "                    \n",
    "    fastaoutfilename = 'merged'+community+composition+runnumber+R+F+'.fasta'\n",
    "    \n",
    "    with open(fastaoutfilename, 'wb') as outfile:\n",
    "        for filename in glob.glob('temp_fasta/*/*/*.fasta'):\n",
    "            if filename == fastaoutfilename:\n",
    "                # don't want to copy the output into the output\n",
    "                continue\n",
    "            with open(filename, 'rb') as readfile:\n",
    "                shutil.copyfileobj(readfile, outfile)\n",
    "    shutil.rmtree('temp_fasta', ignore_errors=False, onerror=None)\n",
    "    \n",
    "\n",
    "    if R!='all':\n",
    "        rallfs = separated[separated.Reverse_trim == R]\n",
    "        separated_dic = pd.Series(rallfs.Taxon.values,rallfs.feature_id.values).to_dict()\n",
    "    else:\n",
    "        separated_dic = pd.Series(separated.Taxon.values, separated.feature_id.values).to_dict()\n",
    "    if F!='all':\n",
    "        fallrs = separated[separated.Forward_trim == F]\n",
    "        separated_dic = pd.Series(fallrs.Taxon.values,fallrs.feature_id.values).to_dict()\n",
    "    else:\n",
    "        separated_dic = pd.Series(separated.Taxon.values, separated.feature_id.values).to_dict()\n",
    "\n",
    "    fa = SeqIO.parse('input_data/'+community+'/'+fastaoutfilename,\n",
    "                 \"fasta\")\n",
    "    seqs_i_want = [] #we'll put the good sequences here\n",
    "    for record in fa: #a SeqRecord has the accession as record.id, usually.\n",
    "        if record.id in separated_dic.keys(): #This is how you check if the accession is in the values of the dict\n",
    "            seqs_i_want.append(record)\n",
    "    #Now we can write the list of records to a fasta file. This will take care of the formatting etc\n",
    "    with open('input_data/'+community+composition+runnumber+R+F+'.fasta', \"w\") as f:\n",
    "        SeqIO.write(seqs_i_want, f, \"fasta\")\n",
    "    \n",
    "    return print('Saved selected sequences as' 'input_data/'+community+'/'+runnumber+composition+'R'+R+'F'+F+'.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__make_tbd_hm__ makes a heatmap showing the taxonomic beta diversity of each trim length combination against the expected community. TBD is a dissimilarity index based on taxonomic trees between two samples where 1 is completely different, and 0 is the completely the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tbd_hm(level=7, truthfilename):\n",
    "    if comm == '02-PROKs':\n",
    "        expected_file = 'expected_16s_staggered'\n",
    "    if comm='02-EUKs':\n",
    "        expected_file = 'expected_18s_staggered'\n",
    "    \n",
    "    bacaros_dm = pd.read_csv(comm+'/Bacaros/results-'+level)\n",
    "    bacaros_dm = bacaros_dm.set_index('Unnamed: 0')\n",
    "    bacaros_dm = 1  - bacaros_dm\n",
    "    #bacaros_dm is a distance matrix of table X table\n",
    "    my_pcoa = skbio.stats.ordination.pcoa(bacaros_dm.values)\n",
    "    plt.scatter(my_pcoa.samples['PC1'],  my_pcoa.samples['PC2'])\n",
    "    against_exp = bacaros_dm[[truthfilename]].copy()\n",
    "    against_exp = against_exp.reset_index().rename(columns={against_exp.index.name:'sample_name'})\n",
    "    against_exp.drop(against_exp.index[against_exp['sample_name'] == truthfilename], inplace=True)\n",
    "    against_exp['Forward_trim'] = [s.split('R')[0] for s in against_exp['sample_name']]\n",
    "    against_exp['Forward_trim'] = [s.split('46F')[1] for s in against_exp['Forward_trim']]\n",
    "    against_exp['Reverse_trim'] = [s.split('R')[1] for s in against_exp['sample_name']]\n",
    "    against_exp[\"Forward_trim\"] = pd.to_numeric(against_exp[\"Forward_trim\"])\n",
    "    against_exp[\"Reverse_trim\"] = pd.to_numeric(against_exp[\"Reverse_trim\"])\n",
    "    against_exp[\"Forward_trim\"].replace({0: 280}, inplace=True)\n",
    "    against_exp[\"Reverse_trim\"].replace({0: 290}, inplace=True)\n",
    "    tohm = against_exp.pivot(\"Forward_trim\", \"Reverse_trim\", \"expected_18s_staggered\")\n",
    "    tohm.rename({280: 'full'}, axis=0, inplace=True)\n",
    "    tohm.rename({290: 'full'}, axis=1, inplace=True)\n",
    "    ax = sns.heatmap(tohm, cmap=sns.color_palette(\"hls\", 90))\n",
    "    ax.invert_yaxis()\n",
    "    plt.figure(figsize=(12,12))\n",
    "    \n",
    "    # get max and min values\n",
    "    print('The min value is {0} and the max value is {1}'.format(df.at[df.stack().index[np.argmin(df.values)]], \n",
    "                                                                 minv = df.at[df.stack().index[np.argmax(df.values)]]))\n",
    "\n",
    "    return (tohm, bacaros_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__r2_plot__ makes a linear regression of the observed relative abundances of each combination of trim lengths against the expected, and plots each coefficient of determination in a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to import the expected taxonomies and transofrm to ratios\n",
    "def r2_plot(path):\n",
    "    expected = pd.read_csv(path, sep='\\t')\n",
    "    expected = expected.rename(columns={'silva_taxonomy':'Taxon', 'sample-id': 'Replicate'})\n",
    "    expected_even = expected[expected.mock_even_insilico != 0]\n",
    "    expected_even = expected_even.drop(columns=['mock_staggered_insilico','taxonomy'])\n",
    "    expected_even.reset_index(drop=True, inplace=True)\n",
    "    expected_even['expected_ratio'] = expected_even['mock_even_insilico']/(expected_even['mock_even_insilico'].sum())\n",
    "    expected_stagg = expected.drop(columns=['mock_even_insilico','taxonomy'])\n",
    "    expected_stagg.reset_index(drop=True, inplace=True)\n",
    "    expected_stagg['expected_ratio'] = expected_stagg['mock_staggered_insilico']/(expected_stagg['mock_staggered_insilico'].sum())\n",
    "    \n",
    "    return (expected_even, expected_stagg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__get_fig_per_group__ makes a heatmap of the observed against expected relative abundances for given group names and colors ratios above the expected in blues, and below the expected in reds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fig_per_group(groupname, expectedratio, duplicate='mean'):\n",
    "    neoceratium = separated[separated['Taxon'].str.contains(groupname)]\n",
    "    neoceratium = neoceratium[neoceratium.feature_frequency !=0]\n",
    "    neoceratium.rename(columns = {'sample-id':'sample_id'}, inplace=True)\n",
    "    if duplicate!='mean':\n",
    "        neoceratiumR1 = neoceratium[neoceratium.sample_id == 'R46-18S-'+duplicate]\n",
    "    else:\n",
    "        neoceratiumR1 = neoceratium.groupby(['Forward_trim','Reverse_trim'])[['ratio']].mean()\n",
    "        neoceratiumR1 = neoceratiumR1.reset_index()\n",
    "    neoceratiumR1[\"Forward_trim\"] = pd.to_numeric(neoceratiumR1[\"Forward_trim\"])\n",
    "    neoceratiumR1[\"Reverse_trim\"] = pd.to_numeric(neoceratiumR1[\"Reverse_trim\"])\n",
    "    neoceratiumR1[\"Forward_trim\"].replace({0: 280}, inplace=True)\n",
    "    neoceratiumR1[\"Reverse_trim\"].replace({0: 290}, inplace=True)\n",
    "    neoceratiumR1merged = neoceratiumR1.groupby(['Forward_trim','Reverse_trim'])[['ratio']].mean()\n",
    "    neoceratiumR1merged = neoceratiumR1merged.reset_index()\n",
    "    tohm = neoceratiumR1merged.pivot(\"Forward_trim\", \"Reverse_trim\", \"ratio\")\n",
    "    tohm.rename({280: 'full'}, axis=0, inplace=True)\n",
    "    tohm.rename({290: 'full'}, axis=1, inplace=True)\n",
    "    ax = sns.heatmap(tohm, cmap=\"Oranges_r\")#, mask= (tohm < (expectedratio-(0.0005*expectedratio))) & (tohm > (expectedratio+(0.0005*expectedratio)))) #cmap=sns.color_palette(\"hls\", 90)\n",
    "    ax = sns.heatmap(tohm, mask=tohm <= expectedratio, cmap=sns.color_palette(\"GnBu\", 5)) #square=True, annot=False, vmin=0, vmax=1, cbar=False, ax=ax)\n",
    "    #ax = sns.heatmap(tohm, mask=tohm >= expectedratio, cmap=sns.color_palette(\"Oranges_r\", 5)) #square=True, annot=False, vmin=0, vmax=1, cbar=False, ax=ax)\n",
    "    ax.invert_yaxis()\n",
    "    plt.figure(figsize=(12,12))\n",
    "    ax.set(xlabel='Reverse trim length', ylabel='Forward trim length')\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig('ratio'+groupname+'.png', bbox_inches = \"tight\")\n",
    "    return neoceratiumR1merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ttst__ runs one sample t tests between each taxonomic groups observed against expected relative abundances, and plots the results in a boxplot for a single trimming combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run 1 sample t test\n",
    "#takes trimcombination in format F40R40\n",
    "def ttst(trimcombination):\n",
    "    compr_obs_exp = expstagg.merge(separated, how='outer', on='Taxon')\n",
    "    compr_obs_exp = compr_obs_exp.rename(columns={'sample-id': 'Replicate'})\n",
    "    compr_obs_exp=compr_obs_exp.fillna(0)\n",
    "    compr_obs_exp[\"group\"].replace({0: \"False positive\"}, inplace=True)\n",
    "    df_with_groups = compr_obs_exp[compr_obs_exp.table_id == trimcombination]\n",
    "    \n",
    "    \n",
    "    means = df_with_groups.groupby('group').mean()\n",
    "    newsi=means[['expected_ratio', 'ratio']].copy()\n",
    "    newsi.sort_values('expected_ratio', ascending = False)\n",
    "    newsi['Difference'] = newsi['expected_ratio'] / newsi['ratio']\n",
    "    newsi\n",
    "    \n",
    "    # generate a boxplot to see the data distribution by treatments. Using boxplot, we can easily detect the differences between different treatments\n",
    "    ax = sns.boxplot(x='ratio', y='group', data=df_with_groups).set(\n",
    "    xlabel='Relative abundance', \n",
    "    ylabel='Group'\n",
    "    )\n",
    "    #ax.tick_params(axis='x', labelrotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "    results = []\n",
    "    for group in groups:    \n",
    "        arr = df_with_groups[(df_with_groups['group'] == group)]['ratio'].values  #Filter the dataframe. \n",
    "        results.append({'group': group,\n",
    "                        'ratios': arr}) #Make a single \"record\" containing the table id, replicate, and ratio array.\n",
    "        r_gr = pd.DataFrame.from_records(results)\n",
    "    \n",
    "    y = expstagg['expected_ratio']\n",
    "    for i in range(len(y[0:-1])):\n",
    "        xi = r_gr.iloc[i]['ratios']\n",
    "        yi = y[i]\n",
    "        print(r_gr.iloc[i]['group'])\n",
    "        print(stats.ttest_1samp(xi, yi))\n",
    "        #but there are outliers\n",
    "    return (r_gr, newsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run analyses and make figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_figures(community={\"16S\", \"18S\"}, composition, runnumber, R='all', F='all',level=7, truthfilename):\n",
    "    df = consolidate_tables(community)\n",
    "    merged = merge_metada()\n",
    "    separated_taxonomies, separated_dic = pick_metadata(composition, runnumber, R='all', F='all')\n",
    "    rename_taxonomies()\n",
    "    make_fasta()\n",
    "    make_tbd_hm(level=7)\n",
    "    r2_plot()\n",
    "    get_fig_per_group()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pluginanalysis",
   "language": "python",
   "name": "pluginanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
